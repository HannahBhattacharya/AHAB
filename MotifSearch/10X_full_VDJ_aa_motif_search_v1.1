#This script:
#1
#Takes single chains export csv file from 10X Genomics 
#Loupe VDJ Browser. Note that this file can be defined
#by "gating" a cluster in Loupe Cell Browser and 
#using that cluster to filter/select which VDJ sequences
#are pulled from the VDJ file. This pulls only the VH
#sequences from that (easy to modify to other chains)

#2
#It generates a list of data frames that contain 
#the barcode (contig_id) and nucleotide (nt) seq
#and identifies best open reading frame (ORF) and
#returns corresponding amino acid sequence

#3
#NOTE THIS FEATURE IS STILL UNDER DEVELOPMENT
#It further searches for matches to a defined aa motif
#and calculates those BCR matches as a % of the total BCRs
#Each barcode becomes a separate row CSV output has 
#filename, chain, clonotype_id, 
#single VH call (single chain), proportion, 
#and VH_family

########################
#SEE ALL CAPS FOR THINGS YOU'RE LIKELY TO NEED
#TO EDIT BASED ON YOUR SPECIFIC NEEDS

library(tidyverse)
library(magrittr)
library(readr)

check.create.dir <- function(the.dir) {
  if (!dir.exists(the.dir)) {
    dir.create(the.dir, recursive = TRUE) }
}

#########################
#DEFINE FILEPATH WHERE LCB (LOUPE CELL BROWSER)
#FILES ARE COMING FROM
dir.in <- "~/desktop/10X_BCR_pipelines/input"

####################
#DEFINE FILEPATH OUT
dir.out <- "~/desktop/10X_BCR_pipelines/REDCap/out/"

directories <- c(dir.in, dir.out)
lapply(directories,
       FUN = check.create.dir)

library(dplyr)
library(tidyr)
library(stringr)

#####################################
#THIS NEEDS TO POINT TO LCB FILE PATH
#THIS IS HOW YOU FILTER DOWN TO B CELL
#SUBSET, E.G. CD21LOW
#"cd21low.+csv" will find any file with the cd21low
# subset name in the csv file name
subset.name <- "all"
file.list <- list.files(dir.in, pattern = "all.+csv",
                        full.names = TRUE)
file.list

#this puts all the data into one list of data frames,
#one df for each samples
data.list <- lapply(file.list, read.csv)
#test <- data.frame(data.list[1])
#pull file id without leading path, subset or .csv
####################
#MAY NEED TO ALTER BASED ON FILE PATH HIERARCHY/NAME
make.sample.names <- function(file) {
  filename <- as.character((data.frame
                            (strsplit(file, "/")) [7,1]))
  filename <- as.character((data.frame
                            (strsplit(filename, "usage_")) [2,1]))
  filename <- as.character((data.frame
                            (strsplit(filename, ".csv")) [1,1]))
  filename <- gsub(subset.name, "", filename)
  filename <- gsub("__", "_", filename)
}
sample.names <- lapply(file.list, make.sample.names)
sample.names

make.file.names <- function(file) {
  filename <- as.character((data.frame
                            (strsplit(file, "/")) [7,1]))
  filename <- as.character((data.frame
                            (strsplit(filename, "usage_")) [2,1]))
  filename <- as.character((data.frame
                            (strsplit(filename, ".csv")) [1,1]))
  file.name <- as.character((data.frame
                            (strsplit(filename, "_")) [1,1]))
  file.name <- gsub(subset.name, "", file.name)
  file.name <- gsub("__", "_", file.name)
}
filenames <- lapply(file.list, make.file.names)
filenames
sample.names <- lapply(file.list, make.sample.names)
sample.names
#insert filename information into each
#data frame in list
data.list <- Map(cbind, data.list, file_id = filenames)
data.list <- Map(cbind, data.list, sample_id = sample.names)

##########################################
#ALTER THIS TO INCLUDE WHATEVER METADATA YOU NEED
#FOR DOWNSTREAM PLOTTING; WHAT GROUPS DO YOU WANT
#TO COMPARE? KEEP TWO GROUPS NAMED AS FDR OR T1D
#OR EDIT ALL DOWNSTREAM VARIABLE NAMES IN CODE
#define dx group based on order of samples in list
#This example uses 6 lanes of 10X data that have
#three hashed samples per lane (total n=18 samples)
#YOU WILL HAVE TO EDIT DOWNSTREAM CODE A BIT
#BASED ON HOW MANY LANES OF 10X DATA YOU HAVE. SORRY.
disease <- c("FDR","T1D","T1D",
             "FDR", "FDR","T1D",
             "FDR","T1D","FDR",
             "T1D","FDR","T1D",
             "T1D","FDR","FDR",
             "T1D","T1D","FDR")

subset.list <- c(subset.name,subset.name,subset.name,
                 subset.name,subset.name,subset.name,
                 subset.name,subset.name,subset.name,
                 subset.name,subset.name,subset.name,
                 subset.name,subset.name,subset.name,
                 subset.name,subset.name,subset.name)

data.list <- Map(cbind, data.list, dx_group = disease)
data.list <- Map(cbind, data.list, subset_name = subset.list)

#this puts hashed files back together so barcodes match
#the lanes they came from. Otherwise there could be 
#two cells from two lanes with same barcode call I think.
#########EDIT file_id ACCORDINGLY
lane1.list <- lapply(data.list, 
                     function(x) 
                       subset(x, file_id == "4025-RB-1"))

lane2.list <- lapply(data.list, 
                     function(x) 
                       subset(x, file_id == "4025-RB-2"))

lane3.list <- lapply(data.list, 
                     function(x) 
                       subset(x, file_id == "4025-RB-3"))

lane4.list <- lapply(data.list, 
                     function(x) 
                       subset(x, file_id == "4025-RB-4"))

lane5.list <- lapply(data.list, 
                     function(x) 
                       subset(x, file_id == "4025-RB-5"))

lane6.list <- lapply(data.list, 
                     function(x) 
                       subset(x, file_id == "4025-RB-6"))

#this creates a new list of data frames that have
#dx_group == FDR; returns zeros for all values in 
#data frames do not have dx_group == FDR (e.g. T1D)
#I can't figure out how to avoid this, just live with them
#they will get tossed in a minute by rbind
#FDR.data.list <- lapply(data.list, function(x) subset(x, dx_group == "FDR"))
#T1D.data.list <- lapply(data.list, function(x) subset(x, dx_group == "T1D"))

#omit X column that we don't need
#eliminating proportion and frequency so they aren't
#inadvertantly used later
lane1.df <- subset(do.call(rbind, lane1.list), 
                   select = -c(X, proportion, frequency))
lane2.df <- subset(do.call(rbind, lane2.list), 
                   select = -c(X, proportion, frequency))
lane3.df <- subset(do.call(rbind, lane3.list), 
                   select = -c(X, proportion, frequency))
lane4.df <- subset(do.call(rbind, lane4.list), 
                   select = -c(X, proportion, frequency))
lane5.df <- subset(do.call(rbind, lane5.list), 
                   select = -c(X, proportion, frequency))
lane6.df <- subset(do.call(rbind, lane6.list), 
                   select = -c(X, proportion, frequency))
#removing things we don't need anymore
rm(data.list, #FDR.data.list, T1D.data.list,
   lane1.list, lane2.list, lane3.list, lane4.list,
   lane5.list, lane6.list, subset.list)

#need to put separate row for each contig_id and repeat
#other data. Otherwise the proportion calls later are
#totally messed up
lane1.df <- separate_rows(lane1.df, contig_ids, sep = ";")
lane2.df <- separate_rows(lane2.df, contig_ids, sep = ";")
lane3.df <- separate_rows(lane3.df, contig_ids, sep = ";")
lane4.df <- separate_rows(lane4.df, contig_ids, sep = ";")
lane5.df <- separate_rows(lane5.df, contig_ids, sep = ";")
lane6.df <- separate_rows(lane6.df, contig_ids, sep = ";")

#filtering on IgH BCRs; 
#YOU COULD EDIT THIS TO IGK or IGL INSTEAD
lane1.vh <- select(filter(lane1.df, chain == "IGH"), c(1:length(lane1.df)))
lane2.vh <- select(filter(lane2.df, chain == "IGH"), c(1:length(lane2.df)))
lane3.vh <- select(filter(lane3.df, chain == "IGH"), c(1:length(lane3.df)))
lane4.vh <- select(filter(lane4.df, chain == "IGH"), c(1:length(lane4.df)))
lane5.vh <- select(filter(lane5.df, chain == "IGH"), c(1:length(lane5.df)))
lane6.vh <- select(filter(lane6.df, chain == "IGH"), c(1:length(lane6.df)))

#DEFINE FILEPATH IN for 10X VDJ files
vdj.dir.in <- list.files(path = "~/10X_genomics/data/4025-RB", 
                         pattern = "filtered_contig_annotations.csv", 
                         recursive = TRUE, full.names = TRUE)
vdj.file.list <- lapply(vdj.dir.in, read.csv)

sample.list <- c("4025-RB-1", "4025-RB-2", "4025-RB-3", 
                 "4025-RB-4", "4025-RB-5", "4025-RB-6")
vdj.file.list <- Map(cbind, vdj.file.list, sample.id = sample.list)

#combine all together into one list of dataframes
df.list <- list(lane1.vh, lane2.vh, lane3.vh,
                lane4.vh, lane5.vh, lane6.vh)

meta.data.df <- do.call(rbind, df.list)
vdj.df <- do.call(rbind, vdj.file.list)
rm(lane1.vh, lane2.vh, lane3.vh, lane4.vh, lane5.vh, lane6.vh,
   lane1.df, lane2.df, lane3.df, lane4.df, lane5.df, lane6.df)

#filter on full-length and productive prior to join
vdj.df <- select(filter(vdj.df, 
              full_length == "True" & productive == "True"), 
       c(1:length(vdj.df)))

#ensure join requires that sample file IDs match, since same
#barcode could be re-used in a separate lane.
ahab <- meta.data.df %>% 
  inner_join(vdj.df,
             by = c("contig_ids" = "contig_id", "file_id" = "sample.id")) 

## still need to join in the actual sequences, but need above steps
#for data QC.

#This series of functions will use input nt seq and identify they best 
#open reading frame (orf) and add a new data frame column to each
#element of the list of data frames generated above

#Note: read.fasta in phylotools pkg will create: ATCG
#whereas read.fasta in seqinr pkg will create: "A" "T" "C" "G"

library(phylotools)
vdj.dir.in <- list.files(path = "~/10X_genomics/data/4025-RB", 
                         pattern = "filtered_contig.fasta", 
                         recursive = TRUE, full.names = TRUE)
vdj.file.list <- lapply(vdj.dir.in, phylotools::read.fasta)

#if (!requireNamespace("BiocManager", quietly = TRUE))
#  install.packages("BiocManager")

library(BiocGenerics)
library(Biostrings)

# This finds and translates longest open reading frame (ORF).
# ATG omitted as a requirement since many BCR seqs may start past ATG;
# minimum stop codons used to identify ORF. Also considers reverse
# complement of sequence, so all 6 reading frames are checked.
find.stops <- function(nt.seq){
  # Define a vector with the sequences of potential stop codons
  codons <- c("taa", "tag", "tga")
  
  # Find the number of occurrences of each type of potential stop codon
  #in fwd sequence
  for (i in 1:3)
    {
    codon <- codons[i]
    # Find all occurrences of codon "codon" in fwd sequence "nt.seq"
    occurrences <- Biostrings::matchPattern(codon, DNAString(nt.seq))

    # Find the start positions of all occurrences of "codon" in "nt.seq"
    #might need to revist to: codonpositions <- attr(occurrences,"start")
    codonpositions <- start(occurrences)
    #codonpositions <- attr(occurrences,"start")
    # Find the total number of potential stop codons in "nt.seq"
    numoccurrences <- length(codonpositions)
    if (i == 1)
    {
      # Make a copy of vector "codonpositions" called "positions"
      positions <- codonpositions
      # Make a vector "types" containing "numoccurrences" copies of "codon"
      types <- rep(codon, numoccurrences)
    }
    else
    {
      # Add the vector "codonpositions" to the end of vector "positions":
      positions <- append(positions, codonpositions, after=length(positions))
      # Add the vector "rep(codon, numoccurrences)" to the end of vector "types":
      types  <- append(types, rep(codon, numoccurrences), after=length(types))
    }
  }
  # Sort the vectors "positions" and "types" in order of position
  # along the input sequence:
  indices <- order(positions)
  positions <- positions[indices]
  types <- types[indices]
  # Return a list variable including vectors "positions" and "types":
  stop.list <- list(positions,types)
  return(stop.list)
}

# Find fwd ORF
find.orf <- function(nt.seq) {
  fwd.stops <- find.stops(nt.seq)
  rc.stops <- find.stops(c2s(rev(comp(s2c(nt.seq)))))
  
  fwd.stop.df <- data.frame("x" = fwd.stops[[1]], "rf" = fwd.stops[[1]] %% 3)
  fwd.rf1 <- sum(fwd.stop.df$rf == 1)
  fwd.rf2 <- sum(fwd.stop.df$rf == 2)
  fwd.rf3 <- sum(fwd.stop.df$rf == 0)
  rc.stop.df <- data.frame("x" = rc.stops[[1]], "rf" = rc.stops[[1]] %% 3)
  rc.rf1 <- sum(rc.stop.df$rf == 1)
  rc.rf2 <- sum(rc.stop.df$rf == 2)
  rc.rf3 <- sum(rc.stop.df$rf == 0)
  #find seq with minimum number of stop codons
  best.orf <- c("f1", "f2", "f3",
                "rc1", "rc2", "rc3")[which.min(c(fwd.rf1, fwd.rf2, fwd.rf3,
                                              rc.rf1, rc.rf2, rc.rf3))]
  return(best.orf)
}

  # translate best reading frame
translate.best.orf <- function(nt.seq){
  best.orf <- find.orf(nt.seq)
  #generate fwd and rc seq in DNAString format,
  #required by downstream function calls
  dna.seq <- DNAString(nt.seq)
  dna.seq.rc <- reverseComplement(dna.seq)
  
  #generate list of all 3 translations
  #based on possible start positions for fwd seqs
  dna.f <- lapply(1:3, function(pos)
    subseq(dna.seq, start=pos))
  dna.f.trans <- lapply(dna.f, Biostrings::translate, no.init.codon = TRUE)
  #generate list of all 3 translations
  #based on possible start positions for rc seqs
  dna.rc <- lapply(1:3, function(pos)
    subseq(dna.seq.rc, start=pos))
  dna.rc.trans <- lapply(dna.rc, Biostrings::translate, no.init.codon = TRUE)
  
  #retrieve best translation based on min number of stop codons
  if (best.orf %in% "f1") {
    dna.trans <- (dna.f.trans[1])
    dna.trans <- as.character(dna.trans[[1]])
    return(dna.trans)
  } else if (best.orf %in% "f2") {
    dna.trans <- (dna.f.trans[2])
    dna.trans <- as.character(dna.trans[[1]])
    return(dna.trans)
  } else if (best.orf %in% "f3") {
    dna.trans<- (dna.f.trans[3])
    dna.trans <- as.character(dna.trans[[1]])
    return(dna.trans)
    #rc
  } else if (best.orf %in% "rc1") {
    dna.trans <- (dna.rc.trans[1])
    dna.trans <- as.character(dna.trans[[1]])
    return(dna.trans)
  } else if (best.orf %in% "rc2") {
    dna.trans <- (dna.rc.trans[2])
    dna.trans <- as.character(dna.trans[[1]])
    return(dna.trans)
  } else if (best.orf %in% "rc3") {
    dna.trans <- (dna.rc.trans[3])
    dna.trans <- as.character(dna.trans[[1]])
    return(dna.trans)
  }
}

#To test above on single sequence, remove comments below:
#test.nt <- "GGTCCTGGGCCCAGTCTGCCCTGACTCAGCCTGCCTCCGTGTCTGGGTCTCCTGGACAGTCGATCACCATCTCCTGCATTGGAACCAGCAGTGACATTGGTGGTTATAACTATGTCTCCTGGTACCAACAACACCCAGGCAAAGCCCCCAAACTCATAATTTATGAGGTCAGTGATCGGCCCTCAGGGGCTTCTAATCGCTTCTCTGGCTCCAAGTCTGGCAACACGGCCTCCCTGACCATCTCTGGGCTCCAGGCTGAAGACGAGGCTGATTATTTTTGCAGCTCACATGCAAGCAGCAGCACTTATGTCTTCGGAACTGGGACCAAGGTCACCGTCCTAGGTCAGCCCAAGGCCAACCCCACTGTCACTCTGTTCCCGCCCTCCTCTGAGGAGCTCCAAGCCAACAAGGCCACACTGGTGA"
#test.rc <- c2s(rev(comp(s2c(test.nt))))
#test.nt
#test.rc

#stops1 <- find.stops(test.nt)
#stops1
#orf1 <- find.orf(test.nt)
#orf1
#best1 <- translate.best.orf(test.nt)
#best1

#stopsrc <- find.stops(test.rc)
#stopsrc
#orfrc <- find.orf(test.rc)
#orfrc
#bestrc <- translate.best.orf(test.rc)
#bestrc

#this adds aa seq as character vector, position aligned
#with nt seq

#this will go pretty slowly so you may want to consider
#running this by list element (uncomment below and
#repeat for each list element if so).
#vdj.file.list[[1]]$aa.text <- sapply(
#  vdj.file.list[[1]]$seq.text, translate.best.orf)

#this is going to take two forevers
for(i in seq_along(vdj.file.list)) {
  i <- 1
  while(i <= length(vdj.file.list)) {
    vdj.file.list[[i]]$aa.text <- sapply(vdj.file.list[[i]]$seq.text, 
                                         translate.best.orf)
    i <- i + 1
  }
  return(vdj.file.list)
}
